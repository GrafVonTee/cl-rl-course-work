import json
import numpy as np
from tqdm.auto import tqdm
from typing import List, Dict
from vllm import SamplingParams

from src.executor import LocalExecutor
from src.metrics import BaseCodeMetric, ExecutionResult
from src.data.types import CodingTask
import src.config as config  # <--- 1. –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥

class Evaluator:
    def __init__(self, llm_engine, tokenizer, metrics: List[BaseCodeMetric]):
        self.llm = llm_engine
        self.tokenizer = tokenizer
        self.metrics = metrics
        self.executor = LocalExecutor()

    def run(self, tasks: List[CodingTask]) -> Dict[str, float]:
        final_results = {}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ä–∞–∑—É –≥–æ—Ç–æ–≤–∏–º SamplingParams
        grouped_configs = self._group_metrics_and_prepare_params()

        for config_key, group in grouped_configs.items():
            sampling_params = group['params']
            metrics_in_group = group['metrics']

            # –õ–æ–≥–∏—Ä—É–µ–º, —Å –∫–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–∞–ø—É—Å–∫–∞–µ–º
            print(f"\nüöÄ Group: {[m.name for m in metrics_in_group]}")
            print(f"‚öôÔ∏è Params: n={sampling_params.n}, temp={sampling_params.temperature}, "
                  f"max_tokens={sampling_params.max_tokens}, logprobs={sampling_params.logprobs}")

            # --- PHASE A: GENERATION ---
            prompts = [t.prompt for t in tasks]

            # vLLM generate
            outputs = self.llm.generate(prompts, sampling_params)

            # --- PHASE B: EXECUTION ---
            # –ù–∞–º –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—ç–º–ø–ª–æ–≤,
            # —á—Ç–æ–±—ã –æ—Ç–¥–∞—Ç—å –∏—Ö –≤ ProcessPoolExecutor –ø–∞—á–∫–æ–π.

            # 1. –°–æ–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á–∏ –≤ —Å–ø–∏—Å–æ–∫
            flat_tasks_input = []     # [(code, tests), ...]
            map_indices = []          # [(task_idx, sample_idx), ...] –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

            for i, request_output in enumerate(outputs):
                task_data = tasks[i]
                for j, sample in enumerate(request_output.outputs):
                    flat_tasks_input.append((sample.text, task_data.tests))
                    map_indices.append((i, j))

            # 2. –ó–ê–ü–£–°–ö–ê–ï–ú –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û (–í—Å—è –º–∞–≥–∏—è —Ç—É—Ç)
            # –≠—Ç–æ –≤–µ—Ä–Ω–µ—Ç —Å–ø–∏—Å–æ–∫ ExecutionResult —Ç–∞–∫–æ–π –∂–µ –¥–ª–∏–Ω—ã, –∫–∞–∫ flat_tasks_input
            flat_results = self.executor.batch_execute(flat_tasks_input)

            # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É (Task -> Samples)
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤
            all_exec_results = [[] for _ in range(len(tasks))]

            # –†–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–æ–ª–æ—á–∫–∞–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é
            for k, exec_res in enumerate(flat_results):
                task_idx, sample_idx = map_indices[k]

                # –î–æ—Å—Ç–∞–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é, –∫–æ—Ç–æ—Ä—É—é –º—ã –º–æ–≥–ª–∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å —Ä–∞–Ω–µ–µ –∏–ª–∏ —Å—á–∏—Ç–∞–µ–º —Å–µ–π—á–∞—Å
                # (–í vLLM outputs —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ outputs)
                original_sample = outputs[task_idx].outputs[sample_idx]
                entropy = self._calculate_entropy(original_sample)
                exec_res.entropy = entropy

                all_exec_results[task_idx].append(exec_res)

            # --- PHASE C: METRICS ---
            for metric in metrics_in_group:
                score = metric.calculate(all_exec_results)
                final_results[metric.name] = score
                print(f"üìä {metric.name}: {score:.4f}")

        return final_results

    def _group_metrics_and_prepare_params(self):
        groups = {}

        # 2. –ë–µ—Ä–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.py
        base_settings = config.SAMPLING_SETTINGS.copy()

        # –£–¥–∞–ª—è–µ–º –∏–∑ –±–∞–∑—ã —Ç–æ, —á—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—Ç –º–µ—Ç—Ä–∏–∫–∏, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        # (vLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'n' –≤–º–µ—Å—Ç–æ 'num_return_sequences')
        base_settings.pop("n", None)
        base_settings.pop("temperature", None)

        for metric in self.metrics:
            # –ö–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–µ—Ç—Ä–∏–∫–∏)
            cfg = metric.gen_config
            cfg_key = json.dumps(cfg, sort_keys=True)

            if cfg_key not in groups:
                # 3. –ú–ï–†–î–ñ–ò–ú: –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ + –°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –º–µ—Ç—Ä–∏–∫–∏

                # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª—é—á–µ–π: Metrics (HF style) -> vLLM style
                n = cfg.get("num_return_sequences", 1)
                temp = cfg.get("temperature", 0.0)

                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç vLLM SamplingParams
                # –ú—ã —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º **base_settings (—Ç–∞–º max_tokens, repetition_penalty –∏ —Ç.–¥.)
                # –ò —è–≤–Ω–æ –∑–∞–¥–∞–µ–º n –∏ temperature
                vllm_params = SamplingParams(
                    n=n,
                    temperature=temp,
                    stop_token_ids=[self.tokenizer.eos_token_id], # –í–∞–∂–Ω–æ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    **base_settings
                )

                groups[cfg_key] = {'params': vllm_params, 'metrics': []}

            groups[cfg_key]['metrics'].append(metric)

        return groups

    def _calculate_entropy(self, sample_output):
        """–°—á–∏—Ç–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –¥–ª—è vLLM outputs"""
        # vLLM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç logprobs –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {token_id: logprob}
        if not sample_output.logprobs:
            return 0.0

        entropies = []
        for step_logprobs in sample_output.logprobs:
            # step_logprobs: Dict[int, Logprob] (top-k tokens)
            if not step_logprobs: continue

            # –î–ª—è —Ç–æ—á–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –Ω—É–∂–Ω—ã –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –Ω–æ vLLM –¥–∞–µ—Ç —Ç–æ–ø-K.
            # –ë–µ—Ä–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –í–´–ë–†–ê–ù–ù–û–ì–û —Ç–æ–∫–µ–Ω–∞ –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.
            # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–∏–µ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.

            # sample_output.token_ids —Å–æ–¥–µ—Ä–∂–∏—Ç id –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ logprobs - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –ø–æ —à–∞–≥–∞–º.
            # vLLM —É—Å—Ç—Ä–æ–µ–Ω–∞ —Ç–∞–∫: step_logprobs[token_id].logprob –¥–∞–µ—Ç –ª–æ–≥-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.

            # –ü—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º logprob —Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –≤—ã–±—Ä–∞–Ω (–æ–Ω –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å –≤ –≤–æ–∑–≤—Ä–∞—Ç–µ, –µ—Å–ª–∏ logprobs=1)
            # –ù–æ –ø—Ä–æ—â–µ –≤–∑—è—Ç—å values(), —Ç–∞–∫ –∫–∞–∫ –º—ã –æ–±—ã—á–Ω–æ –ø—Ä–æ—Å–∏–º logprobs=1, —Ç–∞–º –±—É–¥–µ—Ç 1 –∑–Ω–∞—á–µ–Ω–∏–µ
            val = list(step_logprobs.values())[0].logprob

            # Entropy contribution ~ -log(p) (Surprise)
            entropies.append(-val)

        return np.mean(entropies) if entropies else 0.0
