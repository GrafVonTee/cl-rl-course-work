

=== FILE: src/__init__.py ===


=== FILE: src/inference/vllm_inference.py ===
from functools import partial
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest
import src.config as config

def setup_model(use_sft=False):
    model_path = config.MODEL_PATH
    adapter_path = config.SFT_MODEL_PATH

    llm = LLM(
        model=model_path,
        enable_lora=use_sft,
        max_lora_rank=64 if use_sft else None,
        **config.VLLM_PARAMS
    )

    tokenizer = AutoTokenizer.from_pretrained(
        adapter_path if use_sft else model_path,
        trust_remote_code=True
    )

    if use_sft:
        request = LoRARequest("sft", 1, lora_path=adapter_path)
        llm.generate = partial(llm.generate, lora_request=request)

    sampling_params = SamplingParams(
        stop_token_ids=[tokenizer.eos_token_id, tokenizer.pad_token_id],
        **config.SAMPLING_SETTINGS
    )

    return llm, tokenizer, sampling_params


=== FILE: src/prompt/humaneval.py ===
from src.config import DATASETS_DIR
from datasets import load_dataset
from src.data.types import CodingTask
import re
from typing import List, Optional


def get_dataset():
    return load_dataset("openai/openai_humaneval", cache_dir=DATASETS_DIR)


def get_prepared_dataset(tokenizer):
    humaneval = get_dataset()
    return humaneval.map(build_prompt, fn_kwargs={"tokenizer": tokenizer})


def build_prompt(example: dict, tokenizer) -> str:
    task_id = example.get("task_id", "")
    task_text = example["prompt"].rstrip()
    entry_point = example.get("entry_point", "")

    system_msg = (
        "You are an expert Python coding assistant. "
        "You will be given a Python file snippet containing imports and a single function "
        "signature with a docstring. Complete the function implementation so it is correct."
    )

    user_msg = (
        f"Task ID: {task_id}\n"
        f"Function to implement: {entry_point}\n\n"
        "Complete the following code by writing the function body.\n"
        "- Keep all existing imports, the function name, and its arguments unchanged.\n"
        "- Do not modify the docstring.\n"
        "- You may add local helper functions if needed, but do not change the target signature.\n"
        "- Return ONLY valid Python code (no explanations, no markdown, no code fences).\n\n"
        "Code:\n"
        f"{task_text}\n"
    )

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    return prompt


def extract_tests(test_str: str) -> List[str]:
    """–ü–∞—Ä—Å–∏—Ç asserts –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–µ—Å—Ç–æ–≤ (–¥–ª—è HumanEval)."""
    if not test_str:
        return []

    lines = test_str.splitlines()
    tests: List[str] = []
    i = 0
    while i < len(lines):
        line = lines[i]
        # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ —Å –±–∞–ª–∞–Ω—Å–æ–º —Å–∫–æ–±–æ–∫ –¥–ª—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã—Ö assert
        if re.match(r"^\s*assert\b", line):
            buf = [line.rstrip()]
            i += 1
            text = "\n".join(buf)
            balance = text.count("(") - text.count(")")
            balance += text.count("[") - text.count("]")
            balance += text.count("{") - text.count("}")

            while i < len(lines) and balance > 0:
                buf.append(lines[i].rstrip())
                text = "\n".join(buf)
                balance = text.count("(") - text.count(")")
                balance += text.count("[") - text.count("]")
                balance += text.count("{") - text.count("}")
                i += 1
            tests.append("\n".join(buf).strip())
        else:
            i += 1
    return tests


def humaneval_to_task(row: dict, tokenizer) -> CodingTask:
    prompt_str = build_prompt(row, tokenizer)

    # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –∑–¥–µ—Å—å.
    # –¢–µ–ø–µ—Ä—å executor –ø–æ–ª—É—á–∏—Ç —Å–ø–∏—Å–æ–∫ ['assert x==1', 'assert x==2']
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç candidate = entry_point, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –≤ test_execute
    entry_point = row['entry_point']
    raw_tests = extract_tests(row['test'])
    prepared_tests = [f"candidate = {entry_point}\n{t}" for t in raw_tests]

    return CodingTask(
        task_id=row['task_id'],
        prompt=prompt_str,
        canonical_solution=row['canonical_solution'],
        tests=prepared_tests,
        stop_tokens=["\nclass", "\ndef", "\n#", "if __name__"]
    )


=== FILE: src/prompt/mbpp.py ===
from src.config import DATASETS_DIR
from datasets import load_dataset
from src.data.types import CodingTask


def get_dataset():
    return load_dataset("google-research-datasets/mbpp", "sanitized", cache_dir=DATASETS_DIR)


def get_prepared_dataset(tokenizer, split="train"):
    mbpp = get_dataset()
    return mbpp[split].map(
        build_prompt,
        fn_kwargs={"tokenizer": tokenizer, "train": (split == "train")}
    )


def extract_signature_from_mbpp_code(code: str) -> str:
    for line in code.splitlines():
        line = line.strip()
        if line.startswith("def "):
            return line.rstrip(":")
    raise ValueError("No function signature found")


def build_prompt(example: dict, tokenizer, train=False) -> str:
    task_text = example["prompt"]
    code_solution = example["code"]
    signature_line = extract_signature_from_mbpp_code(example["code"])

    system_msg = (
        "You are an expert Python coding assistant. "
        "Given a problem description and function signature, "
        "implement the function body so that it passes all tests."
    )
    user_msg = (
        "Problem:\n"
        f"{task_text}\n\n"
        "Use the following function signature:\n"
        f"{signature_line}:\n\n"
        "Write the full Python function implementation. "
        "Do NOT change the function name or arguments. "
        "Return only Python code."
    )
    assistant_msg = (
        f"```python\n{code_solution}\n```"
    )

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]
    if train:
        messages.append({"role": "assistant", "content": assistant_msg})

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=not train,
        thinking=True
    )

    return {"text": text}


def mbpp_to_task(row: dict, tokenizer) -> CodingTask:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É MBPP –≤ CodingTask"""
    prompt_data = build_prompt(row, tokenizer, train=False)

    return CodingTask(
        task_id=str(row['task_id']),
        prompt=prompt_data['text'],
        canonical_solution=row['code'],
        tests=row['test_list'], # –í MBPP —ç—Ç–æ —É–∂–µ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
        stop_tokens=["\nclass", "\ndef", "\nif", "\nprint"]
    )


=== FILE: src/data/loader.py ===
from typing import Callable, List, Any
from datasets import Dataset
from .types import CodingTask

def load_benchmark(
    dataset: Any,
    mapper_fn: Callable[[Any], CodingTask]
) -> List[CodingTask]:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –ª—é–±–æ–π –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π dataset –≤ —Å–ø–∏—Å–æ–∫ CodingTask,
    –∏—Å–ø–æ–ª—å–∑—É—è –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é mapper_fn –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏.
    """
    tasks = []
    # –ï—Å–ª–∏ —ç—Ç–æ HF Dataset, –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –Ω–µ–º—É
    iterable = dataset if not isinstance(dataset, dict) else dataset['test']

    for row in iterable:
        try:
            task = mapper_fn(row)
            tasks.append(task)
        except Exception as e:
            print(f"Error processing task {row.get('task_id', '?')}: {e}")

    return tasks


=== FILE: src/data/types.py ===
from dataclasses import dataclass, field
from typing import List

@dataclass
class CodingTask:
    task_id: str
    prompt: str           # –£–∂–µ –≥–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å (—Å —á–∞—Ç-—Ç–µ–º–ø–ª–µ–π—Ç–æ–º)
    canonical_solution: str
    tests: List[str]      # –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è assert
    stop_tokens: List[str] = field(default_factory=list)


=== FILE: src/logger.py ===
import logging
import sys
import os
from src.config import LOGS_DIR

class Colors:
    RESET = "\033[0m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    RED = "\033[91m"

def setup_logger(name: str, log_filename: str = "app.log", level=logging.INFO):
    os.makedirs(LOGS_DIR, exist_ok=True)
    logger = logging.getLogger(name)
    logger.setLevel(level)

    logger.propagate = False
    if logger.hasHandlers():
        logger.handlers.clear()

    # 1. –§–æ—Ä–º–∞—Ç –¥–ª—è —Ñ–∞–π–ª–∞ (–ü–æ–ª–Ω—ã–π)
    file_fmt = logging.Formatter(
        fmt="%(asctime)s | %(levelname)-8s | %(name)-15s :: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # 2. –§–æ—Ä–º–∞—Ç –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏ (–ö—Ä–∞—Ç–∫–∏–π + –¶–≤–µ—Ç–Ω–æ–π)
    # –ú—ã –¥–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–∞ –ø—Ä—è–º–æ –≤ —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞
    console_fmt = logging.Formatter(
        fmt=f"{Colors.BLUE}%(asctime)s{Colors.RESET} | %(levelname)s | %(message)s",
        datefmt="%H:%M:%S"
    )

    # –•–µ–Ω–¥–ª–µ—Ä –∫–æ–Ω—Å–æ–ª–∏
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(console_fmt)
    logger.addHandler(console_handler)

    # –•–µ–Ω–¥–ª–µ—Ä —Ñ–∞–π–ª–∞
    file_path = os.path.join(LOGS_DIR, log_filename)
    file_handler = logging.FileHandler(file_path, mode='a', encoding='utf-8')
    file_handler.setFormatter(file_fmt)
    logger.addHandler(file_handler)

    return logger


=== FILE: src/train/lora_train.py ===
import torch
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import FastLanguageModel
import src.config as config
import src.prompt.mbpp as mbpp
from src.logger import setup_logger


logger = setup_logger(__name__, "training.log")

def train_model():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, –æ–±—É—á–∞–µ—Ç –µ—ë –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (model, tokenizer) –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {config.MODEL_PATH}...")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = config.MODEL_PATH,
        max_seq_length = 2048,
        dtype = None,
        load_in_4bit = True,
    )

    # –ù–∞–≤–µ—à–∏–≤–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã LoRA
    model = FastLanguageModel.get_peft_model(
        model,
        r = 64,
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj"],
        lora_alpha = 64,
        lora_dropout = 0,
        bias = "none",
        use_gradient_checkpointing = "unsloth",
    )

    logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    dataset = mbpp.get_prepared_dataset(tokenizer, split="train")

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = 2048,
        packing = False,
        args = TrainingArguments(
            per_device_train_batch_size = 4,
            gradient_accumulation_steps = 4,
            max_steps = 100,
            learning_rate = 3e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 10,
            output_dir = "checkpoints",
            optim = "adamw_8bit",
            report_to = "none", # –û—Ç–∫–ª—é—á–∞–µ–º wandb
        ),
    )

    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    trainer.train()

    logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ {config.SFT_MODEL_PATH}...")
    model.save_pretrained(config.SFT_MODEL_PATH, tokenizer)
    tokenizer.save_pretrained(config.SFT_MODEL_PATH)

    logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    return model, tokenizer


=== FILE: src/train/ptuning_train.py ===
import torch
from torch.nn import CrossEntropyLoss
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit
import src.config as config
import src.prompt.mbpp as mbpp
from src.logger import setup_logger

logger = setup_logger(__name__, "ptuning.log")

# –ù–∞—à –∫–∞—Å—Ç–æ–º–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        outputs = model(**inputs)

        if hasattr(outputs, "loss") and outputs.loss is not None:
            loss = outputs.loss
        else:
            logits = outputs.get("logits")
            labels = inputs.get("labels")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ labels –≤—Å—ë –µ—â–µ –Ω–µ—Ç
            if labels is None:
                raise ValueError("Labels are missing in inputs! Tokenization failed to create them.")

            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            loss_fct = CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        return (loss, outputs) if return_outputs else loss

def train():
    logger.info(f"Load Base Model: {config.MODEL_NAME}")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    model = AutoModelForCausalLM.from_pretrained(
        config.MODEL_PATH,
        quantization_config=bnb_config,
        device_map="auto"
    )

    model.config.use_cache = False
    model.gradient_checkpointing_enable()

    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    system_msg = (
        "You are an expert Python coding assistant. "
        "Given a problem description and function signature, "
        "implement the function body so that it passes all tests."
    )

    logger.info("Setup P-Tuning...")
    peft_config = PromptTuningConfig(
        task_type=TaskType.CAUSAL_LM,
        num_virtual_tokens=20,
        prompt_tuning_init=PromptTuningInit.TEXT,
        prompt_tuning_init_text=system_msg,
        tokenizer_name_or_path=config.MODEL_PATH,
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    logger.info("Tokenizing...")
    raw_dataset = mbpp.get_prepared_dataset(tokenizer, split="train")

    # === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ===
    def tokenize_lazy(examples):
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=1024,
            padding=False
        )
        # !!! –í–ê–ñ–ù–û: –ö–æ–ø–∏—Ä—É–µ–º input_ids –≤ labels !!!
        # –ë–µ–∑ —ç—Ç–æ–≥–æ –º–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–µ—Ç, —Å —á–µ–º —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    # ============================

    tokenized_dataset = raw_dataset.map(
        tokenize_lazy,
        batched=True,
        remove_columns=raw_dataset.column_names
    )

    logger.info("Start Training...")

    trainer = CustomTrainer(
        model=model,
        train_dataset=tokenized_dataset,
        args=TrainingArguments(
            output_dir=config.PTUNING_MODEL_PATH,
            max_steps=300,
            learning_rate=1e-2,
            logging_steps=10,
            report_to="none",
            fp16=True,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=8,
            gradient_checkpointing=True,
            optim="paged_adamw_8bit",
        ),
        data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True, model=model)
    )

    trainer.train()

    model.save_pretrained(config.PTUNING_MODEL_PATH)
    logger.info(f"Saved to {config.PTUNING_MODEL_PATH}")

    return model, tokenizer

if __name__ == "__main__":
    train()


=== FILE: src/config.py ===
import os

MODELS_DIR = "./models/"
DATASETS_DIR = "./datasets/"
LOGS_DIR = "./logs/"
MODELS = {
	"0.6b": {
		"name": "Qwen/Qwen3-0.6B",
		"path": MODELS_DIR + "qwen3-0.6b"
	},
	"4b-instruct": {
		"name": "Qwen/Qwen3-4B-Instruct-2507",
		"path": MODELS_DIR + "qwen3-4B-instruct-2507"
	},
}
SELECTED_MODEL = "0.6b"
MODEL_NAME = MODELS[SELECTED_MODEL]["name"]
MODEL_PATH = MODELS[SELECTED_MODEL]["path"]
SFT_MODEL_PATH = f"{MODEL_PATH}-sft"
PTUNING_MODEL_PATH = f"{MODEL_PATH}-ptuning"

VLLM_PARAMS = {
	"max_model_len": 2048,
	"dtype": "auto",
	"gpu_memory_utilization": 0.7,
    "enforce_eager": False,
    "seed": 42,
    "enable_prefix_caching": False,
    "trust_remote_code": True,
}

SAMPLING_SETTINGS = {
    "max_tokens": 2048,
    "ignore_eos": False,
    "detokenize": True,
    "logprobs": 1,
    "repetition_penalty": 1,
}

NUM_PROCESSES = 8


=== FILE: src/metrics.py ===
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from dataclasses import dataclass
import numpy as np


@dataclass
class ExecutionResult:
	code: str
	passed_tests: int = 0
	total_tests: int = 0
	logs: str = ""
	entropy: float = 0.0  # <--- –î–û–ë–ê–í–ò–¢–¨ –≠–¢–û –ü–û–õ–ï

	@property
	def is_passed(self) -> bool:
		return self.total_tests > 0 and self.total_tests == self.passed_tests

	@property
	def pass_ratio(self) -> float:
		if self.total_tests == 0: return 0.0
		return self.passed_tests / self.total_tests


class BaseCodeMetric(ABC):
	def __init__(self, name: str, generation_config: Dict[str, Any]):
		self.name = name
		self.gen_config = generation_config
		self.n_samples = generation_config.get("num_return_sequences", 1)

	@abstractmethod
	def calculate(self, results: List[List[ExecutionResult]]) -> float:
		pass

	def get_config(self):
		return self.gen_config


class GreedyPass(BaseCodeMetric):
	def __init__(self):
		super().__init__(
			name="greedy@1",
			generation_config={
				"temperature": 0.0,
				"do_sample": False,
				"num_return_sequences": 1,
			}
		)

	def calculate(self, results: List[List[ExecutionResult]]) -> float:
		passed_count = sum(1 for task_res in results if task_res[0].is_passed)
		return passed_count / len(results)


class PassAtk(BaseCodeMetric):
	def __init__(self, k: int, n_samples: int, temperature: float = 0.6):
		super().__init__(
			name=f"pass@{k} (n={n_samples})",
			generation_config={
				"temperature": temperature,
				"do_sample": True,
				"num_return_sequences": n_samples,
			}
		)
		self.k = k

	def calculate(self, results):
		scores = []
		for task_result in results:
			c = sum(1 for res in task_result if res.is_passed)
			n = len(task_result)

			if c == 0:
				scores.append(0.0)
				continue

			score = 1
			if n - c >= self.k:
				score -= np.prod(1.0 - self.k / np.arange(n - c + 1, n + 1))
			scores.append(score)

		return np.mean(scores)


class PercentPassed(BaseCodeMetric):
	def __init__(self, temperature: float = 0.6):
		super().__init__(
			name="mean_%passed",
			generation_config={
				"temperature": temperature,
				"do_sample": True,
				"num_return_sequences": 1,
			}
		)

	def calculate(self, results):
		all_ratios = []

		for task_results in results:
			for res in task_results:
				all_ratios.append(res.pass_ratio)

		return np.mean(all_ratios) if all_ratios else 0.0


class MeanEntropy(BaseCodeMetric):
    def __init__(self):
        # –ö–æ–Ω—Ñ–∏–≥ —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ —É pass@10 (–æ–±—ã—á–Ω–æ —Å—á–∏—Ç–∞–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é –Ω–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–∏)
        # –õ–∏–±–æ —Å–¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥–æ–Ω
        super().__init__(
			name="mean_entropy",
			generation_config={
				"temperature": 0.6,
				"do_sample": True,
				"num_return_sequences": 1
			}
		)

    def calculate(self, results):
        entropies = []
        for task_res in results:
            for res in task_res:
                entropies.append(res.entropy)
        return np.mean(entropies) if entropies else 0.0


=== FILE: src/executor.py ===
import multiprocessing as mp
import signal
import sys
import re
from typing import List, Tuple, Dict, Any
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm.auto import tqdm
from src.metrics import ExecutionResult
import src.config as config

# --- –£–¢–ò–õ–ò–¢–´ ---

def extract_code_from_completion(text: str) -> str:
    """–í—ã—Ä–µ–∑–∞–µ—Ç –∫–æ–¥ –∏–∑ –±–ª–æ–∫–∞ ```python ... ``` –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞–∫ –µ—Å—Ç—å."""
    text = re.sub(r"<think>[\s\S]*?(?:</think>|$)", "", text, flags=re.DOTALL)
    m = re.search(r"```(?:python)?\s*(.*?)```", text, re.DOTALL)
    if m:
        return m.group(1)
    if "def " in text:
        return text[text.find("def "):]
    return text

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Timeout reached")

# --- –í–û–†–ö–ï–† (–§—É–Ω–∫—Ü–∏—è –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è –¥–ª—è Pickle) ---

def _process_single_sample(args: Tuple[str, List[str], float]) -> ExecutionResult:
    """
    –í–æ—Ä–∫–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—É–ª–∞.
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç (generated_text, tests, timeout).
    """
    generated_text, tests, timeout = args

    # 1. –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–¥–∞
    clean_code = extract_code_from_completion(generated_text)

    passed_count = 0
    total_count = len(tests)
    logs = ""

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º signal.alarm –¥–ª—è —Ç–∞–π–º–∞—É—Ç–∞ –ë–ï–ó —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥-–ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
    # –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ Linux/Mac, –Ω–æ —ç—Ç–æ –≤ 100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ mp.Process.
    if hasattr(signal, "SIGALRM"):
        signal.signal(signal.SIGALRM, timeout_handler)

    # 3. –ò—Å–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –∫–æ–¥ –æ–¥–∏–Ω —Ä–∞–∑, —á—Ç–æ–±—ã –ø–æ–π–º–∞—Ç—å SyntaxError –¥–æ –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤
    try:
        compiled_code = compile(clean_code, "<string>", "exec")
    except Exception as e:
        return ExecutionResult(clean_code, 0, total_count, logs=f"Syntax Error: {e}")

    for test_case in tests:
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º namespace –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–∞, —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ –≤–ª–∏—è–ª–∏ –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞
        # –ù–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –≤–Ω—É—Ç—Ä–∏ compiled_code
        ns = {}

        try:
            # --- –ó–ê–ü–£–°–ö –° –¢–ê–ô–ú–ê–£–¢–û–ú ---
            if hasattr(signal, "SIGALRM"):
                signal.setitimer(signal.ITIMER_REAL, timeout)

            # 1. –ò—Å–ø–æ–ª–Ω—è–µ–º –∫–æ–¥ –º–æ–¥–µ–ª–∏
            exec(compiled_code, ns, ns)
            # 2. –ò—Å–ø–æ–ª–Ω—è–µ–º —Ç–µ—Å—Ç (assert ...)
            exec(test_case, ns, ns)

            # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ - —É—Å–ø–µ—Ö
            if hasattr(signal, "SIGALRM"):
                signal.setitimer(signal.ITIMER_REAL, 0) # –û—Ç–∫–ª—é—á–∞–µ–º —Ç–∞–π–º–µ—Ä

            passed_count += 1

        except TimeoutException:
            # logs += f"Test timed out.\n"
            pass # –ü—Ä–æ—Å—Ç–æ –Ω–µ –∑–∞—Å—á–∏—Ç—ã–≤–∞–µ–º
        except Exception as e:
            # logs += f"Error: {e}\n"
            pass
        finally:
            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –±—É–¥–∏–ª—å–Ω–∏–∫
            if hasattr(signal, "SIGALRM"):
                signal.setitimer(signal.ITIMER_REAL, 0)

    return ExecutionResult(
        code=clean_code,
        passed_tests=passed_count,
        total_tests=total_count,
        logs=logs
    )


# --- –ö–õ–ê–°–° –≠–ö–ó–ï–ö–£–¢–û–†–ê ---

class LocalExecutor:
    def __init__(self, max_workers: int = None):
        # 2. –ò–°–ü–û–õ–¨–ó–£–ï–ú –ö–û–ù–°–¢–ê–ù–¢–£ –ò–ó –ö–û–ù–§–ò–ì–ê
        # –ï—Å–ª–∏ max_workers –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω —è–≤–Ω–æ, –±–µ—Ä–µ–º –∏–∑ config.NUM_PROCESSES.
        # –ï—Å–ª–∏ –∏ —Ç–∞–º –Ω–µ—Ç (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π), —Ñ–æ–ª–±—ç–∫ –Ω–∞ cpu_count
        default_workers = getattr(config, "NUM_PROCESSES", mp.cpu_count())
        self.max_workers = max_workers if max_workers else default_workers

    def batch_execute(self,
                      tasks: List[Tuple[str, List[str]]],
                      timeout_per_test: float = 2.0) -> List[ExecutionResult]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫.
        """
        # –ì–æ—Ç–æ–≤–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
        map_args = [(text, tests, timeout_per_test) for text, tests in tasks]

        print(f"üöÄ Executing {len(tasks)} samples in parallel using {self.max_workers} workers...")

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # list(executor.map(...)) –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results = list(tqdm(
                executor.map(_process_single_sample, map_args),
                total=len(map_args),
                desc="Running Tests"
            ))

        return results

    def execute(self, generated_text: str, tests: List[str]) -> ExecutionResult:
        """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        return _process_single_sample((generated_text, tests, 2.0))


=== FILE: src/evaluator.py ===
import json
import numpy as np
from tqdm.auto import tqdm
from typing import List, Dict
from vllm import SamplingParams

from src.executor import LocalExecutor
from src.metrics import BaseCodeMetric, ExecutionResult
from src.data.types import CodingTask
import src.config as config  # <--- 1. –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥

class Evaluator:
    def __init__(self, llm_engine, tokenizer, metrics: List[BaseCodeMetric]):
        self.llm = llm_engine
        self.tokenizer = tokenizer
        self.metrics = metrics
        self.executor = LocalExecutor()

    def run(self, tasks: List[CodingTask]) -> Dict[str, float]:
        final_results = {}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ä–∞–∑—É –≥–æ—Ç–æ–≤–∏–º SamplingParams
        grouped_configs = self._group_metrics_and_prepare_params()

        for config_key, group in grouped_configs.items():
            sampling_params = group['params']
            metrics_in_group = group['metrics']

            # –õ–æ–≥–∏—Ä—É–µ–º, —Å –∫–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–∞–ø—É—Å–∫–∞–µ–º
            print(f"\nüöÄ Group: {[m.name for m in metrics_in_group]}")
            print(f"‚öôÔ∏è Params: n={sampling_params.n}, temp={sampling_params.temperature}, "
                  f"max_tokens={sampling_params.max_tokens}, logprobs={sampling_params.logprobs}")

            # --- PHASE A: GENERATION ---
            prompts = [t.prompt for t in tasks]

            # vLLM generate
            outputs = self.llm.generate(prompts, sampling_params)

            # --- PHASE B: EXECUTION ---
            # –ù–∞–º –Ω—É–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—ç–º–ø–ª–æ–≤,
            # —á—Ç–æ–±—ã –æ—Ç–¥–∞—Ç—å –∏—Ö –≤ ProcessPoolExecutor –ø–∞—á–∫–æ–π.

            # 1. –°–æ–±–∏—Ä–∞–µ–º –∑–∞–¥–∞—á–∏ –≤ —Å–ø–∏—Å–æ–∫
            flat_tasks_input = []     # [(code, tests), ...]
            map_indices = []          # [(task_idx, sample_idx), ...] –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

            for i, request_output in enumerate(outputs):
                task_data = tasks[i]
                for j, sample in enumerate(request_output.outputs):
                    flat_tasks_input.append((sample.text, task_data.tests))
                    map_indices.append((i, j))

            # 2. –ó–ê–ü–£–°–ö–ê–ï–ú –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û (–í—Å—è –º–∞–≥–∏—è —Ç—É—Ç)
            # –≠—Ç–æ –≤–µ—Ä–Ω–µ—Ç —Å–ø–∏—Å–æ–∫ ExecutionResult —Ç–∞–∫–æ–π –∂–µ –¥–ª–∏–Ω—ã, –∫–∞–∫ flat_tasks_input
            flat_results = self.executor.batch_execute(flat_tasks_input)

            # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É (Task -> Samples)
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤
            all_exec_results = [[] for _ in range(len(tasks))]

            # –†–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–æ–ª–æ—á–∫–∞–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é
            for k, exec_res in enumerate(flat_results):
                task_idx, sample_idx = map_indices[k]

                # –î–æ—Å—Ç–∞–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é, –∫–æ—Ç–æ—Ä—É—é –º—ã –º–æ–≥–ª–∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å —Ä–∞–Ω–µ–µ –∏–ª–∏ —Å—á–∏—Ç–∞–µ–º —Å–µ–π—á–∞—Å
                # (–í vLLM outputs —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ outputs)
                original_sample = outputs[task_idx].outputs[sample_idx]
                entropy = self._calculate_entropy(original_sample)
                exec_res.entropy = entropy

                all_exec_results[task_idx].append(exec_res)

            # --- PHASE C: METRICS ---
            for metric in metrics_in_group:
                score = metric.calculate(all_exec_results)
                final_results[metric.name] = score
                print(f"üìä {metric.name}: {score:.4f}")

        return final_results

    def _group_metrics_and_prepare_params(self):
        groups = {}

        # 2. –ë–µ—Ä–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.py
        base_settings = config.SAMPLING_SETTINGS.copy()

        # –£–¥–∞–ª—è–µ–º –∏–∑ –±–∞–∑—ã —Ç–æ, —á—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É—é—Ç –º–µ—Ç—Ä–∏–∫–∏, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        # (vLLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'n' –≤–º–µ—Å—Ç–æ 'num_return_sequences')
        base_settings.pop("n", None)
        base_settings.pop("temperature", None)

        for metric in self.metrics:
            # –ö–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–µ—Ç—Ä–∏–∫–∏)
            cfg = metric.gen_config
            cfg_key = json.dumps(cfg, sort_keys=True)

            if cfg_key not in groups:
                # 3. –ú–ï–†–î–ñ–ò–ú: –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ + –°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –º–µ—Ç—Ä–∏–∫–∏

                # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª—é—á–µ–π: Metrics (HF style) -> vLLM style
                n = cfg.get("num_return_sequences", 1)
                temp = cfg.get("temperature", 0.0)

                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç vLLM SamplingParams
                # –ú—ã —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º **base_settings (—Ç–∞–º max_tokens, repetition_penalty –∏ —Ç.–¥.)
                # –ò —è–≤–Ω–æ –∑–∞–¥–∞–µ–º n –∏ temperature
                vllm_params = SamplingParams(
                    n=n,
                    temperature=temp,
                    stop_token_ids=[self.tokenizer.eos_token_id], # –í–∞–∂–Ω–æ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    **base_settings
                )

                groups[cfg_key] = {'params': vllm_params, 'metrics': []}

            groups[cfg_key]['metrics'].append(metric)

        return groups

    def _calculate_entropy(self, sample_output):
        """–°—á–∏—Ç–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –¥–ª—è vLLM outputs"""
        # vLLM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç logprobs –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π {token_id: logprob}
        if not sample_output.logprobs:
            return 0.0

        entropies = []
        for step_logprobs in sample_output.logprobs:
            # step_logprobs: Dict[int, Logprob] (top-k tokens)
            if not step_logprobs: continue

            # –î–ª—è —Ç–æ—á–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏ –Ω—É–∂–Ω—ã –≤—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –Ω–æ vLLM –¥–∞–µ—Ç —Ç–æ–ø-K.
            # –ë–µ—Ä–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –í–´–ë–†–ê–ù–ù–û–ì–û —Ç–æ–∫–µ–Ω–∞ –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.
            # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–∏–µ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.

            # sample_output.token_ids —Å–æ–¥–µ—Ä–∂–∏—Ç id –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ logprobs - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –ø–æ —à–∞–≥–∞–º.
            # vLLM —É—Å—Ç—Ä–æ–µ–Ω–∞ —Ç–∞–∫: step_logprobs[token_id].logprob –¥–∞–µ—Ç –ª–æ–≥-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.

            # –ü—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º logprob —Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –≤—ã–±—Ä–∞–Ω (–æ–Ω –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å –≤ –≤–æ–∑–≤—Ä–∞—Ç–µ, –µ—Å–ª–∏ logprobs=1)
            # –ù–æ –ø—Ä–æ—â–µ –≤–∑—è—Ç—å values(), —Ç–∞–∫ –∫–∞–∫ –º—ã –æ–±—ã—á–Ω–æ –ø—Ä–æ—Å–∏–º logprobs=1, —Ç–∞–º –±—É–¥–µ—Ç 1 –∑–Ω–∞—á–µ–Ω–∏–µ
            val = list(step_logprobs.values())[0].logprob

            # Entropy contribution ~ -log(p) (Surprise)
            entropies.append(-val)

        return np.mean(entropies) if entropies else 0.0
