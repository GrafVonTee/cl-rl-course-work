{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f62aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu128 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    }
   ],
   "source": [
    "from src.train.ptuning_train import train\n",
    "import src.benchmarks as benchmarks\n",
    "import src.prompt.mbpp as mbpp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import src.config as config\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import src.test_execute as test_execute\n",
    "import src.metrics as metrics_utils\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2d2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e200251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É...\n",
      "üîó –ü–æ–¥–∫–ª—é—á–∞–µ–º P-Tuning –∞–¥–∞–ø—Ç–µ—Ä...\n",
      "‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Ç–µ—Å—Ç–∞–º!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "\tattn_implementation=\"sdpa\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "print(\"üîó –ü–æ–¥–∫–ª—é—á–∞–µ–º P-Tuning –∞–¥–∞–ø—Ç–µ—Ä...\")\n",
    "# –ü—É—Ç—å –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ç–µ–º, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª train_ptuning.py\n",
    "ptuning_path = config.PTUNING_MODEL_PATH\n",
    "model = PeftModel.from_pretrained(base_model, ptuning_path)\n",
    "\n",
    "print(\"‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Ç–µ—Å—Ç–∞–º!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3f1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b340ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py:2141: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "user\n",
      "Write a python function to reverse a string.\n",
      "assistant\n",
      "<think>\n",
      "Okay, I need to write a Python function to reverse a string. Let me think. The user wants me to create an expert Python assistant. The task is to write a function to reverse a string. So, the function should take a string as input and return the reversed string. \n",
      "\n",
      "First, I should check if the input is empty. If it's empty, return an empty string. Then, reverse the string. But wait, maybe the user wants to reverse the string in a specific way. Oh, the user said \"reverse a string\", so maybe just reverse the characters. \n",
      "\n",
      "Wait, the user might be asking for a function that can reverse any string. So, the function should take a string and return the reversed version. Let me think about the steps. \n",
      "\n",
      "In Python, the string method is ''.join(reversed(s)) or s[::-1]. So, the function could be something like def reverse_string(s): return ''.join(reversed(s)). \n",
      "\n",
      "But maybe the user wants to handle edge cases. For example, if the string is empty, return empty. So, adding a check. Also, maybe the user wants to reverse the string in a specific way, like reversing each character. But the problem says \"reverse a string\", so probably just the standard reversal. \n",
      "\n",
      "So, the function would be straightforward. Let me code that. \n",
      "\n",
      "Wait, but the user mentioned \"expert Python assistant\". So, the function should be an example. So, the code would be:\n",
      "\n",
      "def reverse_string(s):\n",
      "    return ''.join(reversed(s))\n",
      "\n",
      "But maybe the user wants to handle empty strings. So, adding a check. Also, maybe the user wants to reverse the string in a specific way. But the problem says \"reverse a string\", so perhaps just the standard reversal. \n",
      "\n",
      "So, the code would be as simple as that. Let me write that.\n",
      "</think>\n",
      "\n",
      "```python\n",
      "def reverse_string(s):\n",
      "    return ''.join(reversed(s))\n",
      "```\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"<|im_start|>user\\n\"\n",
    "    \"Write a python function to reverse a string.<|im_end|>\\n\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1024, # –î–∞–µ–º –º–µ—Å—Ç–æ –Ω–∞ –ø–æ–¥—É–º–∞—Ç—å\n",
    "\n",
    "        # === –õ–ï–ö–ê–†–°–¢–í–û –û–¢ –ó–ê–¶–ò–ö–õ–ò–í–ê–ù–ò–Ø ===\n",
    "        do_sample=True,        # –í–∫–ª—é—á–∞–µ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–π—Ç–∏ –∏–∑ —Ü–∏–∫–ª–∞)\n",
    "        temperature=0.1,       # –ù–æ –¥–µ—Ä–∂–∏–º –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–º, —á—Ç–æ–±—ã –∫–æ–¥ –±—ã–ª —Ç–æ—á–Ω—ã–º\n",
    "        top_p=0.9,             # –ù–µ–º–Ω–æ–≥–æ —Å–≤–æ–±–æ–¥—ã\n",
    "        repetition_penalty=1, # <--- –ì–õ–ê–í–ù–û–ï: –®—Ç—Ä–∞—Ñ 20% –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã\n",
    "        # ================================\n",
    "\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6c1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –∑–∞–¥–∞—á MBPP/HumanEval\n",
    "# –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Ö –Ω–µ –Ω–∞–ø–∏—à–µ—Ç, –º—ã –¥–æ–±–∞–≤–∏–º –∏—Ö –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ.\n",
    "STANDARD_IMPORTS = \"\"\"\n",
    "import math\n",
    "import cmath\n",
    "import re\n",
    "import sys\n",
    "import heapq\n",
    "import collections\n",
    "import itertools\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\"\"\"\n",
    "\n",
    "def extract_code_clean(text):\n",
    "    \"\"\"\n",
    "    –í—ã—Ä–µ–∑–∞–µ—Ç –∫–æ–¥ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.\n",
    "    –ï—Å–ª–∏ <think> –Ω–µ –∑–∞–∫—Ä—ã—Ç (–æ–±—Ä—ã–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏), —É–¥–∞–ª—è–µ—Ç –≤—Å—ë –¥–æ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏.\n",
    "    \"\"\"\n",
    "    # 1. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —É–¥–∞–ª—è–µ–º <think>\n",
    "    # (?:</think>|$) –æ–∑–Ω–∞—á–∞–µ—Ç \"–∏–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π —Ç–µ–≥, –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ç–µ–∫—Å—Ç–∞\"\n",
    "    text = re.sub(r\"<think>[\\s\\S]*?(?:</think>|$)\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # 2. –ò—â–µ–º Markdown –±–ª–æ–∫–∏ –∫–æ–¥–∞ (```...```)\n",
    "    code_blocks = re.findall(r\"```(?:python)?\\n(.*?)```\", text, re.DOTALL)\n",
    "\n",
    "    if code_blocks:\n",
    "        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –±–ª–æ–∫\n",
    "        clean_code = max(code_blocks, key=len).strip()\n",
    "    elif \"def \" in text:\n",
    "        # –§–æ–ª–±—ç–∫: –∏—â–µ–º def, –Ω–æ —Ç–µ–ø–µ—Ä—å (–ø–æ—Å–ª–µ —à–∞–≥–∞ 1) –º—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —ç—Ç–æ –Ω–µ –º—ã—Å–ª–∏\n",
    "        # –ù–æ —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –º—É—Å–æ—Ä –ø–µ—Ä–µ–¥ def, –æ—Ç—Ä–µ–∑–∞–µ–º –≤—Å—ë –¥–æ –Ω–µ–≥–æ\n",
    "        start_idx = text.find(\"def \")\n",
    "        clean_code = text[start_idx:].strip()\n",
    "    else:\n",
    "        clean_code = \"\"\n",
    "\n",
    "    # 3. –ï—Å–ª–∏ –∫–æ–¥ –ø—É—Å—Ç–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª—å –Ω–µ —É—Å–ø–µ–ª–∞ –µ–≥–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∏–∑-–∑–∞ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å —Ç–µ—Å—Ç—ã –∏–º–ø–æ—Ä—Ç–∞–º–∏\n",
    "    if not clean_code:\n",
    "        return \"\"\n",
    "\n",
    "    # 4. –ü—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º –∏–º–ø–æ—Ä—Ç—ã\n",
    "    return STANDARD_IMPORTS + \"\\n\" + clean_code\n",
    "\n",
    "\n",
    "def calculate_entropy_transformers_single(scores, sequences, input_len, batch_idx):\n",
    "    \"\"\"\n",
    "    –°—á–∏—Ç–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –¥–ª—è –û–î–ù–û–ì–û —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –±–∞—Ç—á–∞.\n",
    "    \"\"\"\n",
    "    # –ó–∞—â–∏—Ç–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã, –µ—Å–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–º–∞–ª–∞—Å—å\n",
    "    if batch_idx >= sequences.shape[0]: return 0.0\n",
    "\n",
    "    generated_ids = sequences[batch_idx, input_len:]\n",
    "    log_probs_sum = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for i, step_logits in enumerate(scores):\n",
    "        if i >= len(generated_ids): break\n",
    "\n",
    "        # step_logits: [Batch, Vocab]\n",
    "        sample_logits = step_logits[batch_idx]\n",
    "        step_logprobs = torch.nn.functional.log_softmax(sample_logits, dim=-1)\n",
    "\n",
    "        chosen_token_id = generated_ids[i]\n",
    "        token_logprob = step_logprobs[chosen_token_id].item()\n",
    "\n",
    "        log_probs_sum += token_logprob\n",
    "        count += 1\n",
    "\n",
    "    if count == 0: return 0.0\n",
    "    return - (log_probs_sum / count)\n",
    "\n",
    "\n",
    "def run_ptuning_mbpp(dataset_split, model, tokenizer, batch_size=8, limit=None):\n",
    "    print(f\"üöÄ –ó–∞–ø—É—Å–∫ P-Tuning –±–µ–Ω—á–º–∞—Ä–∫–∞...\")\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    metrics_all = []\n",
    "    outputs_text = []\n",
    "\n",
    "    dataset = dataset_split\n",
    "    if limit:\n",
    "        dataset = dataset.select(range(limit))\n",
    "\n",
    "    try:\n",
    "        # <--- 2. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç pbar –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª–æ—Å–∫–æ–π\n",
    "        pbar = tqdm(range(0, len(dataset), batch_size), desc=\"Benchmarking\")\n",
    "\n",
    "        for i in pbar:\n",
    "            try:\n",
    "                # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–∞\n",
    "                batch_slice = dataset.select(range(i, min(i + batch_size, len(dataset))))\n",
    "                prompts_text = [\n",
    "                    mbpp.build_prompt(ex, tokenizer, train=False)[\"text\"]\n",
    "                    for ex in batch_slice\n",
    "                ]\n",
    "\n",
    "                inputs = tokenizer(\n",
    "                    prompts_text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=2048\n",
    "                ).to(\"cuda\")\n",
    "\n",
    "                input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "                # 2. –ì–ï–ù–ï–†–ê–¶–ò–Ø —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏\n",
    "                t_start = time.time()  # <--- 3. –°—Ç–∞—Ä—Ç —Ç–∞–π–º–µ—Ä–∞\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=2048,\n",
    "                        do_sample=False,\n",
    "                        repetition_penalty=1.0,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True\n",
    "                    )\n",
    "\n",
    "                t_end = time.time()  # <--- 4. –°—Ç–æ–ø —Ç–∞–π–º–µ—Ä–∞\n",
    "\n",
    "                # <--- 5. –†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º —Å—ç–º–ø–ª–µ (—Å —É—á–µ—Ç–æ–º –ø–∞–¥–¥–∏–Ω–≥–∞ –¥–æ –º–∞–∫—Å. –¥–ª–∏–Ω—ã –≤ –±–∞—Ç—á–µ)\n",
    "                new_tokens_per_sample = outputs.sequences.shape[1] - input_len\n",
    "                # –û–±—â–µ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–∞—Ç—á–µ (GPU –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∞ –∏—Ö –≤—Å–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)\n",
    "                total_tokens_gen = new_tokens_per_sample * outputs.sequences.shape[0]\n",
    "\n",
    "                duration = t_end - t_start\n",
    "                if duration > 0:\n",
    "                    speed = total_tokens_gen / duration\n",
    "                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ –≤ –±–∞—Ä–µ\n",
    "                    pbar.set_postfix({\"speed\": f\"{speed:.2f} tok/s\", \"len\": new_tokens_per_sample})\n",
    "\n",
    "                batch_sequences = outputs.sequences\n",
    "                batch_scores = outputs.scores\n",
    "\n",
    "                # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "                for j, seq in enumerate(batch_sequences):\n",
    "                    generated_tokens = seq[input_len:]\n",
    "                    raw_answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "                    code_to_test = extract_code_clean(raw_answer)\n",
    "                    outputs_text.append(code_to_test)\n",
    "\n",
    "                    if i == 0 and j == 0:\n",
    "                        print(\"-\" * 20)\n",
    "                        print(f\"DEBUG EXTRACTED:\\n{code_to_test[:200]}...\")\n",
    "                        print(\"-\" * 20)\n",
    "\n",
    "                    example = batch_slice[j]\n",
    "                    tests = example[\"test_list\"]\n",
    "                    passed, total = test_execute.run_mbpp_tests_for_sample(code_to_test, tests)\n",
    "\n",
    "                    entropy = calculate_entropy_transformers_single(batch_scores, batch_sequences, input_len, j)\n",
    "\n",
    "                    metrics_all.append({\n",
    "                        \"pass@1\": 1 if passed == total and total > 0 else 0,\n",
    "                        \"%passed\": passed / total if total > 0 else 0,\n",
    "                        \"entropy\": entropy\n",
    "                    })\n",
    "\n",
    "                # 4. –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò\n",
    "                del inputs, outputs, batch_sequences, batch_scores\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüõë –û—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –≤—Ä—É—á–Ω—É—é. –°–æ—Ö—Ä–∞–Ω—è–µ–º, —á—Ç–æ –µ—Å—Ç—å...\")\n",
    "\n",
    "    return metrics_all, outputs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f2b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ó–∞–ø—É—Å–∫ P-Tuning –±–µ–Ω—á–º–∞—Ä–∫–∞...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9c3aa3a7e44e4ca6612939e99c2987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "DEBUG EXTRACTED:\n",
      "\n",
      "import math\n",
      "import cmath\n",
      "import re\n",
      "import sys\n",
      "import heapq\n",
      "import collections\n",
      "import itertools\n",
      "from typing import List, Dict, Tuple, Optional\n",
      "\n",
      "def remove_Occ(s, ch):\n",
      "    if ch not in s:\n",
      "        retur...\n",
      "--------------------\n",
      "\n",
      "==============================\n",
      "üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò P-TUNING:\n",
      "Pass@1:    40.47%\n",
      "Avg Passed:44.39%\n",
      "Samples:   257\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (validation –∏–ª–∏ test)\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–≤–æ–π –∂–µ mbpp.py\n",
    "mbpp_data = mbpp.get_dataset()[\"test\"] # –∏–ª–∏ \"test\"\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º (–æ–≥—Ä–∞–Ω–∏—á–∏–º 20 –ø—Ä–∏–º–µ—Ä–∞–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏, —É–±–µ—Ä–∏ limit=20 –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞)\n",
    "metrics_result, codes = run_ptuning_mbpp(mbpp_data, model, tokenizer, batch_size=4, limit=None)\n",
    "\n",
    "# –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–≤–æ–µ–π —Ñ—É–Ω–∫—Ü–∏–µ–π\n",
    "final_stats = metrics_utils.aggregate_metrics(metrics_result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò P-TUNING:\")\n",
    "print(f\"Pass@1:    {final_stats['mean_pass@1']:.2%}\")\n",
    "print(f\"Avg Passed:{final_stats['mean_%_passed']:.2%}\")\n",
    "print(f\"Samples:   {final_stats['total_samples']}\")\n",
    "print(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
