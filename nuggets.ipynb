{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f723d4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "WARNING 01-16 00:47:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aadec636",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"./datasets\"\n",
    "os.makedirs(DATASETS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b4fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\", cache_dir=DATASETS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bc8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# MODEL_PATH = \"models/qwen3-0.6b\"\n",
    "MODEL_PATH = \"models/qwen3-4B-instruct-2507\"\n",
    "# OUTPUT_MODEL_DIR = \"models/qwen3-0.6b-sft\"\n",
    "OUTPUT_MODEL_DIR = MODEL_PATH + \"-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f855bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.3: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.13.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5213e75800034ea2a6d0b4a1ceb59771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\tmodel_name = MODEL_PATH,\n",
    "\tmax_seq_length=2048,\n",
    "\tdtype=None,\n",
    "\tload_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bdafaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# 2. –ù–∞–≤–µ—à–∏–≤–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0febbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signature_from_mbpp_code(code: str) -> str:\n",
    "    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É, –Ω–∞—á–∏–Ω–∞—é—â—É—é—Å—è —Å def ...\n",
    "    for line in code.splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"def \"):\n",
    "            return line.rstrip(\":\")  # –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –∏–ª–∏ —É–±—Ä–∞—Ç—å :\n",
    "    raise ValueError(\"No function signature found\")\n",
    "\n",
    "\n",
    "def build_mbpp_prompt(example: dict) -> dict:\n",
    "    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: -> dict, —Ç–∞–∫ –∫–∞–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å\n",
    "    task_text = example[\"prompt\"]\n",
    "    code_solution = example[\"code\"]\n",
    "\n",
    "    # –ü—Ä–µ–¥–ø–æ–ª–æ–∂—É, —á—Ç–æ —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≥–¥–µ-—Ç–æ —Ä—è–¥–æ–º\n",
    "    signature_line = extract_signature_from_mbpp_code(example[\"code\"])\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are an expert Python coding assistant. \"\n",
    "        \"Given a problem description and function signature, \"\n",
    "        \"implement the function body so that it passes all tests.\"\n",
    "    )\n",
    "\n",
    "    user_msg = (\n",
    "        \"Problem:\\n\"\n",
    "        f\"{task_text}\\n\\n\"\n",
    "        \"Use the following function signature:\\n\"\n",
    "        f\"{signature_line}:\\n\\n\"\n",
    "        \"Write the full Python function implementation. \"\n",
    "        \"Do NOT change the function name or arguments. \"\n",
    "        \"Return only Python code.\"\n",
    "    )\n",
    "\n",
    "    assistant_msg = (\n",
    "        f\"```python\\n{code_solution}\\n```\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_msg},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_msg},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef1802f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a43b1dc774a4bfda5f88b6b1e2d1d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95c846ecab64b86b0c7d9146790407b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d634fd797e2e42fd877360108c92b085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279e055946754ef5aa24db7b4dcc4716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list', 'text'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list', 'text'],\n",
       "        num_rows: 257\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list', 'text'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "    prompt: Dataset({\n",
       "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list', 'text'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = mbpp.map(build_mbpp_prompt)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efee473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_file': 'Benchmark Questions Verification V2.ipynb',\n",
       " 'task_id': 602,\n",
       " 'prompt': 'Write a python function to find the first repeated character in a given string.',\n",
       " 'code': 'def first_repeated_char(str1):\\n  for index,c in enumerate(str1):\\n    if str1[:index+1].count(c) > 1:\\n      return c',\n",
       " 'test_imports': [],\n",
       " 'test_list': ['assert first_repeated_char(\"abcabc\") == \"a\"',\n",
       "  'assert first_repeated_char(\"abc\") == None',\n",
       "  'assert first_repeated_char(\"123123\") == \"1\"'],\n",
       " 'text': '<|im_start|>system\\nYou are an expert Python coding assistant. Given a problem description and function signature, implement the function body so that it passes all tests.<|im_end|>\\n<|im_start|>user\\nProblem:\\nWrite a python function to find the first repeated character in a given string.\\n\\nUse the following function signature:\\ndef first_repeated_char(str1):\\n\\nWrite the full Python function implementation. Do NOT change the function name or arguments. Return only Python code.<|im_end|>\\n<|im_start|>assistant\\n```python\\ndef first_repeated_char(str1):\\n  for index,c in enumerate(str1):\\n    if str1[:index+1].count(c) > 1:\\n      return c\\n```<|im_end|>\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de0dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eabf519f01e4859ae4ca22231ccd767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. –¢—Ä–µ–Ω–µ—Ä\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        max_steps = 100, # –û–±—É—á–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "        output_dir = \"checkpoints\",\n",
    "        optim = \"adamw_8bit\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d2fcc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 120 | Num Epochs = 13 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 132,120,576 of 4,154,588,672 (3.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:00, Epoch 12/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.096000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.033900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ models/qwen3-4B-instruct-2507-sft...\n",
      "Detected local model directory: /home/pavel/projects/mouse-learning/cl-rl-course-work/models/qwen3-4B-instruct-2507\n",
      "Found HuggingFace hub cache directory: /home/pavel/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 6868.40it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:31<00:00, 10.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/pavel/projects/mouse-learning/cl-rl-course-work/models/qwen3-4B-instruct-2507-sft`\n",
      "–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å–∫–∞–π —Å–≤–æ–π —Å–∫—Ä–∏–ø—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\n"
     ]
    }
   ],
   "source": [
    "print(\"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "trainer.train()\n",
    "\n",
    "# 5. –°–û–•–†–ê–ù–ï–ù–ò–ï (–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –¥–ª—è —Ç–≤–æ–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞)\n",
    "# –ú—ã —Å–ª–∏–≤–∞–µ–º –≤–µ—Å–∞ (Merge) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ–Ω—è—Ç–Ω–æ–º vLLM\n",
    "print(f\"–°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ {OUTPUT_MODEL_DIR}...\")\n",
    "model.save_pretrained_merged(OUTPUT_MODEL_DIR, tokenizer, save_method=\"merged_16bit\")\n",
    "print(\"–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å–∫–∞–π —Å–≤–æ–π —Å–∫—Ä–∏–ø—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
